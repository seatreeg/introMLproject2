{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile util.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets, utils, regularizers, callbacks\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "def exportModel(history, polarity, amsgrad, model, bias, learningRate):\n",
        "  \"\"\"\n",
        "  this function allows us to save the model in its final state\n",
        "  for later experimentation, use, and reproducability storage\n",
        "\n",
        "  NOTE: this function is designed for google colab\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  fileName=\"cifar100_\"+str(polarity)+\"_\"+str(amsgrad)+\"_B\"+str(bias)+\"_LR\"+str(learningRate)+\".keras\"\n",
        "  print(fileName)\n",
        "  model.save(fileName)\n",
        "  #files.download(fileName)\n",
        "\n",
        "\n",
        "def getData(reproSeed=1):\n",
        "  \"\"\"\n",
        "  this function not only grabs the data from the source\n",
        "  but also does a true 3 way split\n",
        "  as a 2 way split is only performed by default\n",
        "  which does not provide validation data\n",
        "\n",
        "  this function also prepares the data in a way\n",
        "  that works well with our model setup\n",
        "  \"\"\"\n",
        "\n",
        "  # first split, from obtaining the data\n",
        "  (trainValImgs, trainValLabels),(testImgs, testLabels)= datasets.cifar100.load_data( )\n",
        "\n",
        "  # normalize the values because 255 format\n",
        "  # makes things difficult\n",
        "  trainValImgs = trainValImgs/255\n",
        "  testImgs = testImgs/255\n",
        "\n",
        "  #flatten to ensure compatable with later functions\n",
        "  trainValLabels = trainValLabels.ravel()\n",
        "  testLabels = testLabels.ravel()\n",
        "\n",
        "\n",
        "  # second split is needed to run testing\n",
        "  trainImgs, valImgs, trainLabels, valLabels = train_test_split(trainValImgs,\n",
        "                                                              trainValLabels,\n",
        "                                                              test_size=0.25,\n",
        "                                                              random_state=reproSeed)\n",
        "\n",
        "  return trainImgs, valImgs, trainLabels, valLabels, testImgs, testLabels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUFKHQIVvcP1",
        "outputId": "de3d6c9a-32eb-4fc6-abbb-84f6e5677359"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqvD5CYRt_jY",
        "outputId": "ecf00ad9-81a6-4c5f-87b4-bab6b37eed2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets, utils, regularizers, callbacks\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "\n",
        "def modelMaker(amsgrad= False, polarity = \"up\", bias=False, learningRate=0.001):\n",
        "  \"\"\"\n",
        "  this function creates and compiles a model\n",
        "\n",
        "  there are two settings for the architecture:\n",
        "    'up'   : the convolutional layers build up in filter count\n",
        "    'down' : the convolutional layers build down on filter count\n",
        "    (all kernels are the same size)\n",
        "\n",
        "  there are two settings for hyperparamters\n",
        "    'True' : ams grad is on\n",
        "    'False': ams grad is off\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  match polarity:\n",
        "    case \"up\":\n",
        "      model = models.Sequential([\n",
        "        layers.Input(shape=(32, 32, 3)),\n",
        "        layers.BatchNormalization(),\n",
        "        #layers.Conv2D(32, 3, strides=2, padding='same', use_bias=False),\n",
        "\n",
        "        layers.Conv2D(32, 3, padding='same', use_bias=bias), #128\n",
        "        layers.Activation('swish'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(64, 3, padding='same', use_bias=bias), #128\n",
        "        layers.Activation('swish'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(128, 3, padding='same', use_bias=bias), #128\n",
        "        layers.Activation('swish'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "\n",
        "        layers.Flatten(),\n",
        "\n",
        "\n",
        "        layers.Dense(256, activation=\"silu\", kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.Dense(128, activation=\"silu\", kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.Dense(100, activation=\"softmax\")\n",
        "      ])\n",
        "      adamm=Adam(amsgrad=amsgrad, learning_rate=learningRate)\n",
        "      model.compile(optimizer=adamm,\n",
        "                loss=\"sparse_categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "      return model\n",
        "\n",
        "\n",
        "    case \"down\":\n",
        "      model = models.Sequential([\n",
        "        layers.Input(shape=(32, 32, 3)),\n",
        "        layers.BatchNormalization(),\n",
        "        #layers.Conv2D(32, 3, strides=2, padding='same', use_bias=False),\n",
        "\n",
        "        layers.Conv2D(128, 3, padding='same', use_bias=bias), #128\n",
        "        layers.Activation('swish'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(64, 3, padding='same', use_bias=bias), #128\n",
        "        layers.Activation('swish'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(32, 3, padding='same', use_bias=bias), #128\n",
        "        layers.Activation('swish'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "\n",
        "        layers.Flatten(),\n",
        "\n",
        "\n",
        "        layers.Dense(256, activation=\"silu\", kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.Dense(128, activation=\"silu\", kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.Dense(100, activation=\"softmax\")\n",
        "      ])\n",
        "      adamm=Adam(amsgrad=amsgrad, learning_rate=learningRate)\n",
        "      model.compile(optimizer=adamm,\n",
        "                loss=\"sparse_categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "      return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.py\n",
        "\n",
        "from util import *\n",
        "from model import *\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets, utils, regularizers, callbacks\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mainrun(polarity, amsgrad, bias,learningRate):\n",
        "  \"\"\"\n",
        "  this function is the main run for setting up\n",
        "  fitting, testing, and exporting our model\n",
        "\n",
        "  there are some manipulatable settings\n",
        "  but nothing that should be changed between tests\n",
        "  \"\"\"\n",
        "  # seed for reproducability\n",
        "  reproSeed=1\n",
        "  # max epochs\n",
        "  epoccs=100#20#00\n",
        "  # early stopping, does not always activate\n",
        "  # due to other regularizers at play\n",
        "  callback = callbacks.EarlyStopping(monitor='loss',\n",
        "                                                patience=5,\n",
        "                                                restore_best_weights=True)\n",
        "\n",
        "  # get cifar100 fine data with full 3 way split\n",
        "  trainImgs, valImgs, trainLabels, valLabels, testImgs, testLabels = getData(reproSeed=reproSeed)\n",
        "\n",
        "  # generate the model with architecture based on up or down\n",
        "  # and amsgrad on or off\n",
        "  model = modelMaker(amsgrad=amsgrad , polarity=polarity, bias=bias,learningRate=learningRate)\n",
        "\n",
        "  # show relavant information about the model\n",
        "  print(\"polarity:\", polarity,\"   amsgrad:\",  amsgrad)\n",
        "  model.summary()\n",
        "\n",
        "  # fit the model\n",
        "  history = model.fit(\n",
        "    trainImgs, trainLabels, epochs=epoccs, validation_data=(valImgs, valLabels),\n",
        "    callbacks=[callback], batch_size=50\n",
        "  )\n",
        "\n",
        "  # download the model so you can use it later\n",
        "  exportModel(history=history, polarity=polarity, amsgrad=amsgrad, model=model, bias=bias, learningRate=learningRate)\n",
        "\n",
        "  # placeholder until we get resampled CI testacc\n",
        "  testLoss, testAcc = model.evaluate(testImgs, testLabels)\n",
        "  print(f\"Test Accuracy: {testAcc*100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  \"\"\"\n",
        "  each of these function calls is a new test\n",
        "  we need four total\n",
        "  (two architectures two hyperparameters)\n",
        "  more can be added, and individual ones can\n",
        "  be commed out\n",
        "  since these models take a long time to train\n",
        "\n",
        "  This setup helps maintain that we change\n",
        "  what we are interested in for the experiment\n",
        "  and hold all else constant\n",
        "  \"\"\"\n",
        "\n",
        "  #mainrun(polarity=\"up\", amsgrad=False,bias=False,learningRate=0.001)\n",
        "  #mainrun(polarity=\"up\", amsgrad=True, bias=False,learningRate=0.001)\n",
        "  #mainrun(polarity=\"down\", amsgrad=False,bias=False,learningRate=0.001)\n",
        "  #mainrun(polarity=\"down\", amsgrad=True,bias=False,learningRate=0.001)\n",
        "\n",
        "  #mainrun(polarity=\"up\", amsgrad=False,bias=True,learningRate=0.001)\n",
        "  #mainrun(polarity=\"up\", amsgrad=True, bias=True,learningRate=0.001)\n",
        "  #mainrun(polarity=\"down\", amsgrad=False,bias=True,learningRate=0.001)\n",
        "  #mainrun(polarity=\"down\", amsgrad=True,bias=True,learningRate=0.001)\n",
        "\n",
        "  mainrun(polarity=\"up\", amsgrad=False,bias=False,learningRate=0.0001)\n",
        "  mainrun(polarity=\"up\", amsgrad=True, bias=False,learningRate=0.0001)\n",
        "  mainrun(polarity=\"down\", amsgrad=False,bias=False,learningRate=0.0001)\n",
        "  mainrun(polarity=\"down\", amsgrad=True,bias=False,learningRate=0.0001)\n",
        "\n",
        "  #mainrun(polarity=\"up\", amsgrad=False,bias=True,learningRate=0.0001)\n",
        "  #mainrun(polarity=\"up\", amsgrad=True, bias=True,learningRate=0.0001)\n",
        "  #mainrun(polarity=\"down\", amsgrad=False,bias=True,learningRate=0.0001)\n",
        "  #mainrun(polarity=\"down\", amsgrad=True,bias=True,learningRate=0.0001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UfILMXtvDsc",
        "outputId": "dcb1e09d-aff9-4179-d69d-bf7435f9245c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPcJDqOq2ILS",
        "outputId": "3d378f70-f03f-46a8-b44d-ac80ef4f63af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-11 17:40:01.050452: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:95] Opening library: /usr/local/lib/python3.10/dist-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2\n",
            "2024-07-11 17:40:01.050619: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:119] Libtpu path is: libtpu.so\n",
            "2024-07-11 17:40:01.110970: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 2s 0us/step\n",
            "2024-07-11 17:40:17.206759: I external/local_xla/xla/service/service.cc:168] XLA service 0x5abc5c269eb0 initialized for platform TPU (this does not guarantee that XLA will be used). Devices:\n",
            "2024-07-11 17:40:17.206815: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): TPU, 2a886c8\n",
            "2024-07-11 17:40:17.206829: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): TPU, 2a886c8\n",
            "2024-07-11 17:40:17.206840: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (2): TPU, 2a886c8\n",
            "2024-07-11 17:40:17.206849: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (3): TPU, 2a886c8\n",
            "2024-07-11 17:40:17.206859: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (4): TPU, 2a886c8\n",
            "2024-07-11 17:40:17.206868: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (5): TPU, 2a886c8\n",
            "2024-07-11 17:40:17.206878: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (6): TPU, 2a886c8\n",
            "2024-07-11 17:40:17.206888: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (7): TPU, 2a886c8\n",
            "2024-07-11 17:40:17.207037: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207100: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207158: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207213: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207262: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207420: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207516: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207596: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207660: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207724: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.207957: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208049: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208126: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208215: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208270: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208461: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208548: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208630: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208703: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208776: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.208955: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.209043: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.209137: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.209207: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.209285: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.209475: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.209572: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.209644: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.209700: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.209776: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.210040: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.210113: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.210241: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.210300: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.210435: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.210670: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.210759: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.210871: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.211002: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "2024-07-11 17:40:17.211067: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n",
            "polarity: up    amsgrad: False\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization (Batch  (None, 32, 32, 3)         12        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        864       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32, 32, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 64)        18432     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 128)         73728     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               524544    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               12900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 664272 (2.53 MB)\n",
            "Trainable params: 663818 (2.53 MB)\n",
            "Non-trainable params: 454 (1.77 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "750/750 [==============================] - 28s 35ms/step - loss: 8.9883 - accuracy: 0.0689 - val_loss: 7.3493 - val_accuracy: 0.1190\n",
            "Epoch 2/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 6.2962 - accuracy: 0.1562 - val_loss: 5.6043 - val_accuracy: 0.1764\n",
            "Epoch 3/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 4.9993 - accuracy: 0.2157 - val_loss: 4.8195 - val_accuracy: 0.2019\n",
            "Epoch 4/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 4.3038 - accuracy: 0.2567 - val_loss: 4.3226 - val_accuracy: 0.2258\n",
            "Epoch 5/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 3.8665 - accuracy: 0.2914 - val_loss: 4.0081 - val_accuracy: 0.2567\n",
            "Epoch 6/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 3.5649 - accuracy: 0.3203 - val_loss: 3.6572 - val_accuracy: 0.2934\n",
            "Epoch 7/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 3.3387 - accuracy: 0.3452 - val_loss: 3.4833 - val_accuracy: 0.3130\n",
            "Epoch 8/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 3.1721 - accuracy: 0.3681 - val_loss: 3.3479 - val_accuracy: 0.3230\n",
            "Epoch 9/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 3.0422 - accuracy: 0.3833 - val_loss: 3.1833 - val_accuracy: 0.3506\n",
            "Epoch 10/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.9368 - accuracy: 0.4001 - val_loss: 3.1429 - val_accuracy: 0.3493\n",
            "Epoch 11/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.8482 - accuracy: 0.4133 - val_loss: 3.0812 - val_accuracy: 0.3570\n",
            "Epoch 12/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.7788 - accuracy: 0.4238 - val_loss: 2.9934 - val_accuracy: 0.3808\n",
            "Epoch 13/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.7136 - accuracy: 0.4359 - val_loss: 2.9586 - val_accuracy: 0.3778\n",
            "Epoch 14/100\n",
            "750/750 [==============================] - 25s 33ms/step - loss: 2.6585 - accuracy: 0.4476 - val_loss: 2.8653 - val_accuracy: 0.3989\n",
            "Epoch 15/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.6016 - accuracy: 0.4580 - val_loss: 2.8356 - val_accuracy: 0.3971\n",
            "Epoch 16/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.5552 - accuracy: 0.4678 - val_loss: 2.8298 - val_accuracy: 0.4028\n",
            "Epoch 17/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.5140 - accuracy: 0.4763 - val_loss: 2.8073 - val_accuracy: 0.4082\n",
            "Epoch 18/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.4757 - accuracy: 0.4813 - val_loss: 2.7357 - val_accuracy: 0.4204\n",
            "Epoch 19/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.4317 - accuracy: 0.4927 - val_loss: 2.7448 - val_accuracy: 0.4212\n",
            "Epoch 20/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.4019 - accuracy: 0.4963 - val_loss: 2.7374 - val_accuracy: 0.4233\n",
            "Epoch 21/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.3674 - accuracy: 0.5057 - val_loss: 2.6783 - val_accuracy: 0.4354\n",
            "Epoch 22/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.3431 - accuracy: 0.5129 - val_loss: 2.6574 - val_accuracy: 0.4373\n",
            "Epoch 23/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.3174 - accuracy: 0.5187 - val_loss: 2.6563 - val_accuracy: 0.4430\n",
            "Epoch 24/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.2788 - accuracy: 0.5266 - val_loss: 2.6221 - val_accuracy: 0.4467\n",
            "Epoch 25/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.2549 - accuracy: 0.5302 - val_loss: 2.6133 - val_accuracy: 0.4526\n",
            "Epoch 26/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.2216 - accuracy: 0.5367 - val_loss: 2.6536 - val_accuracy: 0.4442\n",
            "Epoch 27/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.2092 - accuracy: 0.5414 - val_loss: 2.5964 - val_accuracy: 0.4571\n",
            "Epoch 28/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.1824 - accuracy: 0.5470 - val_loss: 2.5692 - val_accuracy: 0.4625\n",
            "Epoch 29/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.1605 - accuracy: 0.5535 - val_loss: 2.5844 - val_accuracy: 0.4595\n",
            "Epoch 30/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.1322 - accuracy: 0.5609 - val_loss: 2.5608 - val_accuracy: 0.4678\n",
            "Epoch 31/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.1241 - accuracy: 0.5626 - val_loss: 2.5213 - val_accuracy: 0.4731\n",
            "Epoch 32/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.0913 - accuracy: 0.5693 - val_loss: 2.5185 - val_accuracy: 0.4785\n",
            "Epoch 33/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.0671 - accuracy: 0.5783 - val_loss: 2.5181 - val_accuracy: 0.4771\n",
            "Epoch 34/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.0504 - accuracy: 0.5832 - val_loss: 2.5353 - val_accuracy: 0.4767\n",
            "Epoch 35/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.0388 - accuracy: 0.5855 - val_loss: 2.5570 - val_accuracy: 0.4735\n",
            "Epoch 36/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 2.0287 - accuracy: 0.5871 - val_loss: 2.4962 - val_accuracy: 0.4836\n",
            "Epoch 37/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.9975 - accuracy: 0.5941 - val_loss: 2.4992 - val_accuracy: 0.4823\n",
            "Epoch 38/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.9809 - accuracy: 0.6015 - val_loss: 2.4674 - val_accuracy: 0.4899\n",
            "Epoch 39/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.9622 - accuracy: 0.6061 - val_loss: 2.5119 - val_accuracy: 0.4808\n",
            "Epoch 40/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.9435 - accuracy: 0.6092 - val_loss: 2.4833 - val_accuracy: 0.4905\n",
            "Epoch 41/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.9359 - accuracy: 0.6113 - val_loss: 2.4926 - val_accuracy: 0.4910\n",
            "Epoch 42/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.9219 - accuracy: 0.6164 - val_loss: 2.4956 - val_accuracy: 0.4909\n",
            "Epoch 43/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.8961 - accuracy: 0.6233 - val_loss: 2.5435 - val_accuracy: 0.4814\n",
            "Epoch 44/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.8908 - accuracy: 0.6238 - val_loss: 2.4774 - val_accuracy: 0.4978\n",
            "Epoch 45/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.8703 - accuracy: 0.6305 - val_loss: 2.4987 - val_accuracy: 0.4906\n",
            "Epoch 46/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.8521 - accuracy: 0.6364 - val_loss: 2.4795 - val_accuracy: 0.4974\n",
            "Epoch 47/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.8478 - accuracy: 0.6353 - val_loss: 2.4569 - val_accuracy: 0.5014\n",
            "Epoch 48/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.8284 - accuracy: 0.6381 - val_loss: 2.4916 - val_accuracy: 0.4942\n",
            "Epoch 49/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.8087 - accuracy: 0.6477 - val_loss: 2.4810 - val_accuracy: 0.4913\n",
            "Epoch 50/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.8047 - accuracy: 0.6456 - val_loss: 2.4936 - val_accuracy: 0.4958\n",
            "Epoch 51/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.7864 - accuracy: 0.6534 - val_loss: 2.4560 - val_accuracy: 0.5062\n",
            "Epoch 52/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.7672 - accuracy: 0.6583 - val_loss: 2.4663 - val_accuracy: 0.5034\n",
            "Epoch 53/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.7559 - accuracy: 0.6651 - val_loss: 2.4492 - val_accuracy: 0.5039\n",
            "Epoch 54/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.7608 - accuracy: 0.6590 - val_loss: 2.4610 - val_accuracy: 0.5037\n",
            "Epoch 55/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.7234 - accuracy: 0.6736 - val_loss: 2.5016 - val_accuracy: 0.5048\n",
            "Epoch 56/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.7270 - accuracy: 0.6712 - val_loss: 2.5129 - val_accuracy: 0.4970\n",
            "Epoch 57/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.7057 - accuracy: 0.6788 - val_loss: 2.5007 - val_accuracy: 0.5032\n",
            "Epoch 58/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.7009 - accuracy: 0.6774 - val_loss: 2.4862 - val_accuracy: 0.5077\n",
            "Epoch 59/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.6904 - accuracy: 0.6813 - val_loss: 2.4644 - val_accuracy: 0.5101\n",
            "Epoch 60/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.6740 - accuracy: 0.6866 - val_loss: 2.4712 - val_accuracy: 0.5101\n",
            "Epoch 61/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.6647 - accuracy: 0.6887 - val_loss: 2.4801 - val_accuracy: 0.5059\n",
            "Epoch 62/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.6507 - accuracy: 0.6921 - val_loss: 2.5056 - val_accuracy: 0.5060\n",
            "Epoch 63/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.6541 - accuracy: 0.6917 - val_loss: 2.4870 - val_accuracy: 0.5075\n",
            "Epoch 64/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6276 - accuracy: 0.6985 - val_loss: 2.4985 - val_accuracy: 0.5050\n",
            "Epoch 65/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.6220 - accuracy: 0.7006 - val_loss: 2.4958 - val_accuracy: 0.5081\n",
            "Epoch 66/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6116 - accuracy: 0.7010 - val_loss: 2.5025 - val_accuracy: 0.5122\n",
            "Epoch 67/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.5930 - accuracy: 0.7094 - val_loss: 2.5101 - val_accuracy: 0.5082\n",
            "Epoch 68/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.5899 - accuracy: 0.7117 - val_loss: 2.5005 - val_accuracy: 0.5134\n",
            "Epoch 69/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5814 - accuracy: 0.7129 - val_loss: 2.4853 - val_accuracy: 0.5159\n",
            "Epoch 70/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.5697 - accuracy: 0.7154 - val_loss: 2.4891 - val_accuracy: 0.5128\n",
            "Epoch 71/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.5640 - accuracy: 0.7164 - val_loss: 2.5170 - val_accuracy: 0.5098\n",
            "Epoch 72/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.5567 - accuracy: 0.7196 - val_loss: 2.4998 - val_accuracy: 0.5150\n",
            "Epoch 73/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.5426 - accuracy: 0.7238 - val_loss: 2.5501 - val_accuracy: 0.5057\n",
            "Epoch 74/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.5373 - accuracy: 0.7256 - val_loss: 2.5271 - val_accuracy: 0.5127\n",
            "Epoch 75/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.5215 - accuracy: 0.7292 - val_loss: 2.5307 - val_accuracy: 0.5129\n",
            "Epoch 76/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.5212 - accuracy: 0.7316 - val_loss: 2.5093 - val_accuracy: 0.5132\n",
            "Epoch 77/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.5015 - accuracy: 0.7347 - val_loss: 2.5241 - val_accuracy: 0.5140\n",
            "Epoch 78/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.5067 - accuracy: 0.7345 - val_loss: 2.5569 - val_accuracy: 0.5061\n",
            "Epoch 79/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.4851 - accuracy: 0.7417 - val_loss: 2.5387 - val_accuracy: 0.5125\n",
            "Epoch 80/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.4879 - accuracy: 0.7392 - val_loss: 2.5394 - val_accuracy: 0.5128\n",
            "Epoch 81/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4744 - accuracy: 0.7444 - val_loss: 2.5115 - val_accuracy: 0.5165\n",
            "Epoch 82/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.4679 - accuracy: 0.7444 - val_loss: 2.5299 - val_accuracy: 0.5134\n",
            "Epoch 83/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.4610 - accuracy: 0.7475 - val_loss: 2.5463 - val_accuracy: 0.5078\n",
            "Epoch 84/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.4531 - accuracy: 0.7482 - val_loss: 2.5301 - val_accuracy: 0.5158\n",
            "Epoch 85/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.4421 - accuracy: 0.7531 - val_loss: 2.5286 - val_accuracy: 0.5127\n",
            "Epoch 86/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.4376 - accuracy: 0.7527 - val_loss: 2.5767 - val_accuracy: 0.5092\n",
            "Epoch 87/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.4274 - accuracy: 0.7571 - val_loss: 2.5667 - val_accuracy: 0.5118\n",
            "Epoch 88/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.4254 - accuracy: 0.7570 - val_loss: 2.5781 - val_accuracy: 0.5122\n",
            "Epoch 89/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.4156 - accuracy: 0.7599 - val_loss: 2.5369 - val_accuracy: 0.5134\n",
            "Epoch 90/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.4122 - accuracy: 0.7616 - val_loss: 2.5339 - val_accuracy: 0.5162\n",
            "Epoch 91/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.3994 - accuracy: 0.7651 - val_loss: 2.5448 - val_accuracy: 0.5166\n",
            "Epoch 92/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.3986 - accuracy: 0.7634 - val_loss: 2.5755 - val_accuracy: 0.5139\n",
            "Epoch 93/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.3849 - accuracy: 0.7671 - val_loss: 2.5477 - val_accuracy: 0.5119\n",
            "Epoch 94/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.3759 - accuracy: 0.7729 - val_loss: 2.5752 - val_accuracy: 0.5065\n",
            "Epoch 95/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.3676 - accuracy: 0.7722 - val_loss: 2.5886 - val_accuracy: 0.5121\n",
            "Epoch 96/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.3602 - accuracy: 0.7758 - val_loss: 2.6187 - val_accuracy: 0.5084\n",
            "Epoch 97/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.3615 - accuracy: 0.7744 - val_loss: 2.5627 - val_accuracy: 0.5162\n",
            "Epoch 98/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.3548 - accuracy: 0.7749 - val_loss: 2.5954 - val_accuracy: 0.5127\n",
            "Epoch 99/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.3525 - accuracy: 0.7770 - val_loss: 2.5717 - val_accuracy: 0.5179\n",
            "Epoch 100/100\n",
            "750/750 [==============================] - 25s 34ms/step - loss: 1.3514 - accuracy: 0.7754 - val_loss: 2.5780 - val_accuracy: 0.5109\n",
            "cifar100_up_False_BFalse_LR0.0001.keras\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 2.5311 - accuracy: 0.5236\n",
            "Test Accuracy: 52.36%\n",
            "polarity: up    amsgrad: True\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization_4 (Bat  (None, 32, 32, 3)         12        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 32, 32, 32)        864       \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 32, 32, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 16, 16, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 16, 16, 64)        18432     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         73728     \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 4, 4, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               524544    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 100)               12900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 664272 (2.53 MB)\n",
            "Trainable params: 663818 (2.53 MB)\n",
            "Non-trainable params: 454 (1.77 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "750/750 [==============================] - 28s 35ms/step - loss: 8.9690 - accuracy: 0.0651 - val_loss: 7.2343 - val_accuracy: 0.1202\n",
            "Epoch 2/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 6.2165 - accuracy: 0.1577 - val_loss: 5.4712 - val_accuracy: 0.1912\n",
            "Epoch 3/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 4.9657 - accuracy: 0.2135 - val_loss: 4.6434 - val_accuracy: 0.2247\n",
            "Epoch 4/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 4.2870 - accuracy: 0.2574 - val_loss: 4.1506 - val_accuracy: 0.2561\n",
            "Epoch 5/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 3.8677 - accuracy: 0.2892 - val_loss: 3.8374 - val_accuracy: 0.2738\n",
            "Epoch 6/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 3.5705 - accuracy: 0.3220 - val_loss: 3.6555 - val_accuracy: 0.2895\n",
            "Epoch 7/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 3.3617 - accuracy: 0.3422 - val_loss: 3.4125 - val_accuracy: 0.3177\n",
            "Epoch 8/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 3.1901 - accuracy: 0.3633 - val_loss: 3.3864 - val_accuracy: 0.3168\n",
            "Epoch 9/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 3.0577 - accuracy: 0.3838 - val_loss: 3.2187 - val_accuracy: 0.3378\n",
            "Epoch 10/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.9532 - accuracy: 0.3963 - val_loss: 3.1422 - val_accuracy: 0.3538\n",
            "Epoch 11/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.8666 - accuracy: 0.4085 - val_loss: 3.0513 - val_accuracy: 0.3653\n",
            "Epoch 12/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.7927 - accuracy: 0.4236 - val_loss: 2.9821 - val_accuracy: 0.3778\n",
            "Epoch 13/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.7226 - accuracy: 0.4336 - val_loss: 2.9575 - val_accuracy: 0.3843\n",
            "Epoch 14/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.6753 - accuracy: 0.4428 - val_loss: 2.8852 - val_accuracy: 0.3937\n",
            "Epoch 15/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.6178 - accuracy: 0.4539 - val_loss: 2.8186 - val_accuracy: 0.4058\n",
            "Epoch 16/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 2.5759 - accuracy: 0.4675 - val_loss: 2.7829 - val_accuracy: 0.4106\n",
            "Epoch 17/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 2.5291 - accuracy: 0.4738 - val_loss: 2.8157 - val_accuracy: 0.4057\n",
            "Epoch 18/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.4991 - accuracy: 0.4775 - val_loss: 2.7928 - val_accuracy: 0.4154\n",
            "Epoch 19/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.4557 - accuracy: 0.4873 - val_loss: 2.6939 - val_accuracy: 0.4338\n",
            "Epoch 20/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.4244 - accuracy: 0.4934 - val_loss: 2.7006 - val_accuracy: 0.4334\n",
            "Epoch 21/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.3932 - accuracy: 0.4992 - val_loss: 2.6550 - val_accuracy: 0.4423\n",
            "Epoch 22/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.3581 - accuracy: 0.5097 - val_loss: 2.6814 - val_accuracy: 0.4396\n",
            "Epoch 23/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.3359 - accuracy: 0.5137 - val_loss: 2.6437 - val_accuracy: 0.4414\n",
            "Epoch 24/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.2997 - accuracy: 0.5223 - val_loss: 2.6150 - val_accuracy: 0.4526\n",
            "Epoch 25/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.2769 - accuracy: 0.5295 - val_loss: 2.6460 - val_accuracy: 0.4442\n",
            "Epoch 26/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.2509 - accuracy: 0.5315 - val_loss: 2.5980 - val_accuracy: 0.4558\n",
            "Epoch 27/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.2273 - accuracy: 0.5396 - val_loss: 2.5704 - val_accuracy: 0.4627\n",
            "Epoch 28/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.2096 - accuracy: 0.5433 - val_loss: 2.5689 - val_accuracy: 0.4627\n",
            "Epoch 29/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 2.1836 - accuracy: 0.5512 - val_loss: 2.5392 - val_accuracy: 0.4694\n",
            "Epoch 30/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.1599 - accuracy: 0.5535 - val_loss: 2.5090 - val_accuracy: 0.4767\n",
            "Epoch 31/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.1385 - accuracy: 0.5618 - val_loss: 2.5080 - val_accuracy: 0.4724\n",
            "Epoch 32/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.1240 - accuracy: 0.5678 - val_loss: 2.5426 - val_accuracy: 0.4746\n",
            "Epoch 33/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.1020 - accuracy: 0.5679 - val_loss: 2.5060 - val_accuracy: 0.4782\n",
            "Epoch 34/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.0857 - accuracy: 0.5737 - val_loss: 2.5479 - val_accuracy: 0.4718\n",
            "Epoch 35/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.0617 - accuracy: 0.5803 - val_loss: 2.5175 - val_accuracy: 0.4768\n",
            "Epoch 36/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.0488 - accuracy: 0.5823 - val_loss: 2.4769 - val_accuracy: 0.4842\n",
            "Epoch 37/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 2.0259 - accuracy: 0.5903 - val_loss: 2.5316 - val_accuracy: 0.4778\n",
            "Epoch 38/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 2.0163 - accuracy: 0.5937 - val_loss: 2.4846 - val_accuracy: 0.4887\n",
            "Epoch 39/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.9982 - accuracy: 0.5973 - val_loss: 2.4951 - val_accuracy: 0.4799\n",
            "Epoch 40/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.9757 - accuracy: 0.6015 - val_loss: 2.4852 - val_accuracy: 0.4903\n",
            "Epoch 41/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.9673 - accuracy: 0.6077 - val_loss: 2.4732 - val_accuracy: 0.4918\n",
            "Epoch 42/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.9519 - accuracy: 0.6093 - val_loss: 2.4569 - val_accuracy: 0.4990\n",
            "Epoch 43/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.9328 - accuracy: 0.6168 - val_loss: 2.4581 - val_accuracy: 0.4960\n",
            "Epoch 44/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.9195 - accuracy: 0.6174 - val_loss: 2.4678 - val_accuracy: 0.4931\n",
            "Epoch 45/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.9082 - accuracy: 0.6240 - val_loss: 2.4881 - val_accuracy: 0.4978\n",
            "Epoch 46/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.8900 - accuracy: 0.6266 - val_loss: 2.4930 - val_accuracy: 0.4943\n",
            "Epoch 47/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.8842 - accuracy: 0.6289 - val_loss: 2.4795 - val_accuracy: 0.4979\n",
            "Epoch 48/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.8580 - accuracy: 0.6370 - val_loss: 2.4489 - val_accuracy: 0.5010\n",
            "Epoch 49/100\n",
            "750/750 [==============================] - 26s 34ms/step - loss: 1.8514 - accuracy: 0.6363 - val_loss: 2.4725 - val_accuracy: 0.5018\n",
            "Epoch 50/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.8408 - accuracy: 0.6403 - val_loss: 2.4704 - val_accuracy: 0.5042\n",
            "Epoch 51/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.8250 - accuracy: 0.6480 - val_loss: 2.4521 - val_accuracy: 0.5035\n",
            "Epoch 52/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 1.8111 - accuracy: 0.6488 - val_loss: 2.4550 - val_accuracy: 0.5076\n",
            "Epoch 53/100\n",
            "750/750 [==============================] - 27s 36ms/step - loss: 1.8007 - accuracy: 0.6518 - val_loss: 2.4481 - val_accuracy: 0.5085\n",
            "Epoch 54/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.7843 - accuracy: 0.6566 - val_loss: 2.4816 - val_accuracy: 0.5019\n",
            "Epoch 55/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.7815 - accuracy: 0.6603 - val_loss: 2.4431 - val_accuracy: 0.5061\n",
            "Epoch 56/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.7636 - accuracy: 0.6643 - val_loss: 2.4594 - val_accuracy: 0.5074\n",
            "Epoch 57/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.7566 - accuracy: 0.6641 - val_loss: 2.4531 - val_accuracy: 0.5090\n",
            "Epoch 58/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.7347 - accuracy: 0.6741 - val_loss: 2.4858 - val_accuracy: 0.5083\n",
            "Epoch 59/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.7270 - accuracy: 0.6755 - val_loss: 2.4632 - val_accuracy: 0.5114\n",
            "Epoch 60/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 1.7149 - accuracy: 0.6774 - val_loss: 2.4449 - val_accuracy: 0.5107\n",
            "Epoch 61/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.7075 - accuracy: 0.6762 - val_loss: 2.4516 - val_accuracy: 0.5114\n",
            "Epoch 62/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6987 - accuracy: 0.6824 - val_loss: 2.4754 - val_accuracy: 0.5080\n",
            "Epoch 63/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6829 - accuracy: 0.6861 - val_loss: 2.4762 - val_accuracy: 0.5096\n",
            "Epoch 64/100\n",
            "750/750 [==============================] - 27s 36ms/step - loss: 1.6716 - accuracy: 0.6866 - val_loss: 2.4915 - val_accuracy: 0.5087\n",
            "Epoch 65/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6636 - accuracy: 0.6926 - val_loss: 2.4628 - val_accuracy: 0.5156\n",
            "Epoch 66/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6636 - accuracy: 0.6935 - val_loss: 2.4729 - val_accuracy: 0.5100\n",
            "Epoch 67/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6489 - accuracy: 0.6959 - val_loss: 2.4758 - val_accuracy: 0.5157\n",
            "Epoch 68/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6338 - accuracy: 0.7020 - val_loss: 2.4817 - val_accuracy: 0.5120\n",
            "Epoch 69/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6342 - accuracy: 0.7033 - val_loss: 2.4561 - val_accuracy: 0.5186\n",
            "Epoch 70/100\n",
            "750/750 [==============================] - 27s 36ms/step - loss: 1.6204 - accuracy: 0.7042 - val_loss: 2.4901 - val_accuracy: 0.5151\n",
            "Epoch 71/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.6150 - accuracy: 0.7076 - val_loss: 2.4613 - val_accuracy: 0.5184\n",
            "Epoch 72/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5953 - accuracy: 0.7146 - val_loss: 2.4610 - val_accuracy: 0.5185\n",
            "Epoch 73/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5928 - accuracy: 0.7151 - val_loss: 2.4791 - val_accuracy: 0.5127\n",
            "Epoch 74/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5836 - accuracy: 0.7162 - val_loss: 2.4748 - val_accuracy: 0.5170\n",
            "Epoch 75/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 1.5737 - accuracy: 0.7174 - val_loss: 2.4634 - val_accuracy: 0.5205\n",
            "Epoch 76/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 1.5614 - accuracy: 0.7234 - val_loss: 2.5164 - val_accuracy: 0.5107\n",
            "Epoch 77/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5633 - accuracy: 0.7235 - val_loss: 2.4982 - val_accuracy: 0.5140\n",
            "Epoch 78/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5477 - accuracy: 0.7293 - val_loss: 2.4902 - val_accuracy: 0.5182\n",
            "Epoch 79/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5490 - accuracy: 0.7255 - val_loss: 2.4608 - val_accuracy: 0.5202\n",
            "Epoch 80/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5335 - accuracy: 0.7337 - val_loss: 2.4893 - val_accuracy: 0.5182\n",
            "Epoch 81/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5288 - accuracy: 0.7349 - val_loss: 2.5066 - val_accuracy: 0.5162\n",
            "Epoch 82/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5199 - accuracy: 0.7357 - val_loss: 2.5121 - val_accuracy: 0.5142\n",
            "Epoch 83/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5168 - accuracy: 0.7354 - val_loss: 2.4907 - val_accuracy: 0.5186\n",
            "Epoch 84/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.5098 - accuracy: 0.7378 - val_loss: 2.4878 - val_accuracy: 0.5211\n",
            "Epoch 85/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4938 - accuracy: 0.7434 - val_loss: 2.5009 - val_accuracy: 0.5150\n",
            "Epoch 86/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4870 - accuracy: 0.7456 - val_loss: 2.5237 - val_accuracy: 0.5103\n",
            "Epoch 87/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4845 - accuracy: 0.7445 - val_loss: 2.5009 - val_accuracy: 0.5166\n",
            "Epoch 88/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 1.4722 - accuracy: 0.7494 - val_loss: 2.5011 - val_accuracy: 0.5151\n",
            "Epoch 89/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4714 - accuracy: 0.7516 - val_loss: 2.5063 - val_accuracy: 0.5196\n",
            "Epoch 90/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4622 - accuracy: 0.7514 - val_loss: 2.5080 - val_accuracy: 0.5183\n",
            "Epoch 91/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4537 - accuracy: 0.7552 - val_loss: 2.5114 - val_accuracy: 0.5219\n",
            "Epoch 92/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4494 - accuracy: 0.7549 - val_loss: 2.4980 - val_accuracy: 0.5220\n",
            "Epoch 93/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4407 - accuracy: 0.7570 - val_loss: 2.5361 - val_accuracy: 0.5126\n",
            "Epoch 94/100\n",
            "750/750 [==============================] - 27s 35ms/step - loss: 1.4378 - accuracy: 0.7618 - val_loss: 2.5331 - val_accuracy: 0.5147\n",
            "Epoch 95/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4294 - accuracy: 0.7600 - val_loss: 2.5167 - val_accuracy: 0.5152\n",
            "Epoch 96/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4217 - accuracy: 0.7633 - val_loss: 2.5277 - val_accuracy: 0.5149\n",
            "Epoch 97/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4195 - accuracy: 0.7637 - val_loss: 2.5079 - val_accuracy: 0.5162\n",
            "Epoch 98/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4089 - accuracy: 0.7683 - val_loss: 2.5435 - val_accuracy: 0.5141\n",
            "Epoch 99/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.4055 - accuracy: 0.7685 - val_loss: 2.5247 - val_accuracy: 0.5231\n",
            "Epoch 100/100\n",
            "750/750 [==============================] - 26s 35ms/step - loss: 1.3931 - accuracy: 0.7704 - val_loss: 2.4979 - val_accuracy: 0.5223\n",
            "cifar100_up_True_BFalse_LR0.0001.keras\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 2.4514 - accuracy: 0.5315\n",
            "Test Accuracy: 53.15%\n",
            "polarity: down    amsgrad: False\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization_8 (Bat  (None, 32, 32, 3)         12        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 32, 32, 128)       3456      \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 32, 32, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 16, 16, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 16, 16, 64)        73728     \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 16, 16, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 8, 8, 32)          18432     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 8, 8, 32)          128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPoolin  (None, 4, 4, 32)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 4, 4, 32)          0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100)               12900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 273648 (1.04 MB)\n",
            "Trainable params: 273194 (1.04 MB)\n",
            "Non-trainable params: 454 (1.77 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "750/750 [==============================] - 44s 57ms/step - loss: 8.4090 - accuracy: 0.0466 - val_loss: 7.1149 - val_accuracy: 0.1031\n",
            "Epoch 2/100\n",
            "750/750 [==============================] - 43s 58ms/step - loss: 6.2218 - accuracy: 0.1231 - val_loss: 5.5476 - val_accuracy: 0.1605\n",
            "Epoch 3/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 5.0708 - accuracy: 0.1732 - val_loss: 4.7241 - val_accuracy: 0.1999\n",
            "Epoch 4/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 4.4280 - accuracy: 0.2138 - val_loss: 4.2225 - val_accuracy: 0.2318\n",
            "Epoch 5/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 4.0345 - accuracy: 0.2391 - val_loss: 3.9153 - val_accuracy: 0.2514\n",
            "Epoch 6/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 3.7624 - accuracy: 0.2641 - val_loss: 3.6774 - val_accuracy: 0.2762\n",
            "Epoch 7/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 3.5635 - accuracy: 0.2822 - val_loss: 3.5099 - val_accuracy: 0.2912\n",
            "Epoch 8/100\n",
            "750/750 [==============================] - 43s 57ms/step - loss: 3.4123 - accuracy: 0.2988 - val_loss: 3.3145 - val_accuracy: 0.3198\n",
            "Epoch 9/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 3.2904 - accuracy: 0.3132 - val_loss: 3.3113 - val_accuracy: 0.3083\n",
            "Epoch 10/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 3.1986 - accuracy: 0.3265 - val_loss: 3.1821 - val_accuracy: 0.3260\n",
            "Epoch 11/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 3.1182 - accuracy: 0.3353 - val_loss: 3.0671 - val_accuracy: 0.3418\n",
            "Epoch 12/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 3.0459 - accuracy: 0.3458 - val_loss: 3.0309 - val_accuracy: 0.3399\n",
            "Epoch 13/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 2.9918 - accuracy: 0.3513 - val_loss: 2.9488 - val_accuracy: 0.3616\n",
            "Epoch 14/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.9424 - accuracy: 0.3581 - val_loss: 2.8885 - val_accuracy: 0.3658\n",
            "Epoch 15/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.9000 - accuracy: 0.3663 - val_loss: 2.9104 - val_accuracy: 0.3522\n",
            "Epoch 16/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.8544 - accuracy: 0.3713 - val_loss: 2.8280 - val_accuracy: 0.3732\n",
            "Epoch 17/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.8139 - accuracy: 0.3774 - val_loss: 2.8151 - val_accuracy: 0.3825\n",
            "Epoch 18/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.7792 - accuracy: 0.3829 - val_loss: 2.7730 - val_accuracy: 0.3860\n",
            "Epoch 19/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.7476 - accuracy: 0.3912 - val_loss: 2.7343 - val_accuracy: 0.3908\n",
            "Epoch 20/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.7179 - accuracy: 0.3952 - val_loss: 2.7040 - val_accuracy: 0.3961\n",
            "Epoch 21/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.6868 - accuracy: 0.3994 - val_loss: 2.6982 - val_accuracy: 0.3984\n",
            "Epoch 22/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.6661 - accuracy: 0.4050 - val_loss: 2.6542 - val_accuracy: 0.4058\n",
            "Epoch 23/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.6438 - accuracy: 0.4070 - val_loss: 2.6510 - val_accuracy: 0.4041\n",
            "Epoch 24/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.6190 - accuracy: 0.4129 - val_loss: 2.6346 - val_accuracy: 0.4080\n",
            "Epoch 25/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.6026 - accuracy: 0.4141 - val_loss: 2.5950 - val_accuracy: 0.4183\n",
            "Epoch 26/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.5710 - accuracy: 0.4225 - val_loss: 2.5998 - val_accuracy: 0.4101\n",
            "Epoch 27/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.5564 - accuracy: 0.4223 - val_loss: 2.5648 - val_accuracy: 0.4193\n",
            "Epoch 28/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.5371 - accuracy: 0.4263 - val_loss: 2.5358 - val_accuracy: 0.4237\n",
            "Epoch 29/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.5205 - accuracy: 0.4296 - val_loss: 2.5338 - val_accuracy: 0.4202\n",
            "Epoch 30/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.5003 - accuracy: 0.4351 - val_loss: 2.5315 - val_accuracy: 0.4271\n",
            "Epoch 31/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 2.4906 - accuracy: 0.4338 - val_loss: 2.5110 - val_accuracy: 0.4250\n",
            "Epoch 32/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.4695 - accuracy: 0.4400 - val_loss: 2.5043 - val_accuracy: 0.4302\n",
            "Epoch 33/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.4586 - accuracy: 0.4412 - val_loss: 2.4887 - val_accuracy: 0.4343\n",
            "Epoch 34/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 2.4374 - accuracy: 0.4474 - val_loss: 2.4817 - val_accuracy: 0.4341\n",
            "Epoch 35/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 2.4270 - accuracy: 0.4482 - val_loss: 2.4541 - val_accuracy: 0.4406\n",
            "Epoch 36/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 2.4094 - accuracy: 0.4479 - val_loss: 2.4469 - val_accuracy: 0.4449\n",
            "Epoch 37/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.3972 - accuracy: 0.4518 - val_loss: 2.4326 - val_accuracy: 0.4442\n",
            "Epoch 38/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 2.3833 - accuracy: 0.4591 - val_loss: 2.4483 - val_accuracy: 0.4414\n",
            "Epoch 39/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.3680 - accuracy: 0.4591 - val_loss: 2.4067 - val_accuracy: 0.4494\n",
            "Epoch 40/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.3591 - accuracy: 0.4598 - val_loss: 2.4047 - val_accuracy: 0.4525\n",
            "Epoch 41/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 2.3422 - accuracy: 0.4659 - val_loss: 2.4179 - val_accuracy: 0.4476\n",
            "Epoch 42/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.3339 - accuracy: 0.4679 - val_loss: 2.3909 - val_accuracy: 0.4549\n",
            "Epoch 43/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.3206 - accuracy: 0.4692 - val_loss: 2.3945 - val_accuracy: 0.4502\n",
            "Epoch 44/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.2973 - accuracy: 0.4753 - val_loss: 2.3800 - val_accuracy: 0.4558\n",
            "Epoch 45/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.2944 - accuracy: 0.4741 - val_loss: 2.3628 - val_accuracy: 0.4608\n",
            "Epoch 46/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.2832 - accuracy: 0.4784 - val_loss: 2.3442 - val_accuracy: 0.4614\n",
            "Epoch 47/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.2787 - accuracy: 0.4778 - val_loss: 2.3393 - val_accuracy: 0.4594\n",
            "Epoch 48/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.2653 - accuracy: 0.4790 - val_loss: 2.3480 - val_accuracy: 0.4621\n",
            "Epoch 49/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.2460 - accuracy: 0.4859 - val_loss: 2.3174 - val_accuracy: 0.4714\n",
            "Epoch 50/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 2.2393 - accuracy: 0.4906 - val_loss: 2.3248 - val_accuracy: 0.4711\n",
            "Epoch 51/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.2396 - accuracy: 0.4867 - val_loss: 2.3207 - val_accuracy: 0.4688\n",
            "Epoch 52/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 2.2218 - accuracy: 0.4925 - val_loss: 2.3165 - val_accuracy: 0.4705\n",
            "Epoch 53/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 2.2148 - accuracy: 0.4945 - val_loss: 2.3301 - val_accuracy: 0.4674\n",
            "Epoch 54/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 2.2081 - accuracy: 0.4951 - val_loss: 2.3059 - val_accuracy: 0.4726\n",
            "Epoch 55/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 2.1971 - accuracy: 0.4979 - val_loss: 2.3151 - val_accuracy: 0.4705\n",
            "Epoch 56/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.1851 - accuracy: 0.5007 - val_loss: 2.2787 - val_accuracy: 0.4819\n",
            "Epoch 57/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.1745 - accuracy: 0.5047 - val_loss: 2.2829 - val_accuracy: 0.4780\n",
            "Epoch 58/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.1670 - accuracy: 0.5031 - val_loss: 2.2824 - val_accuracy: 0.4771\n",
            "Epoch 59/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 2.1569 - accuracy: 0.5081 - val_loss: 2.3010 - val_accuracy: 0.4705\n",
            "Epoch 60/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.1456 - accuracy: 0.5070 - val_loss: 2.2836 - val_accuracy: 0.4795\n",
            "Epoch 61/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.1432 - accuracy: 0.5101 - val_loss: 2.2735 - val_accuracy: 0.4790\n",
            "Epoch 62/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.1291 - accuracy: 0.5146 - val_loss: 2.2785 - val_accuracy: 0.4766\n",
            "Epoch 63/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.1240 - accuracy: 0.5133 - val_loss: 2.2450 - val_accuracy: 0.4851\n",
            "Epoch 64/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1074 - accuracy: 0.5170 - val_loss: 2.2665 - val_accuracy: 0.4823\n",
            "Epoch 65/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.1039 - accuracy: 0.5206 - val_loss: 2.2591 - val_accuracy: 0.4887\n",
            "Epoch 66/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0940 - accuracy: 0.5228 - val_loss: 2.2493 - val_accuracy: 0.4879\n",
            "Epoch 67/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 2.0919 - accuracy: 0.5239 - val_loss: 2.2643 - val_accuracy: 0.4774\n",
            "Epoch 68/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0823 - accuracy: 0.5238 - val_loss: 2.2454 - val_accuracy: 0.4892\n",
            "Epoch 69/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0774 - accuracy: 0.5277 - val_loss: 2.2592 - val_accuracy: 0.4825\n",
            "Epoch 70/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0713 - accuracy: 0.5234 - val_loss: 2.2305 - val_accuracy: 0.4934\n",
            "Epoch 71/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0555 - accuracy: 0.5310 - val_loss: 2.2293 - val_accuracy: 0.4949\n",
            "Epoch 72/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0493 - accuracy: 0.5325 - val_loss: 2.2383 - val_accuracy: 0.4897\n",
            "Epoch 73/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0507 - accuracy: 0.5321 - val_loss: 2.2284 - val_accuracy: 0.4962\n",
            "Epoch 74/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0383 - accuracy: 0.5373 - val_loss: 2.2198 - val_accuracy: 0.4958\n",
            "Epoch 75/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0297 - accuracy: 0.5373 - val_loss: 2.2231 - val_accuracy: 0.4951\n",
            "Epoch 76/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0267 - accuracy: 0.5389 - val_loss: 2.2321 - val_accuracy: 0.4892\n",
            "Epoch 77/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0147 - accuracy: 0.5397 - val_loss: 2.1899 - val_accuracy: 0.5056\n",
            "Epoch 78/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0085 - accuracy: 0.5408 - val_loss: 2.2080 - val_accuracy: 0.5001\n",
            "Epoch 79/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0011 - accuracy: 0.5434 - val_loss: 2.2123 - val_accuracy: 0.4962\n",
            "Epoch 80/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9985 - accuracy: 0.5461 - val_loss: 2.2211 - val_accuracy: 0.4984\n",
            "Epoch 81/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9921 - accuracy: 0.5465 - val_loss: 2.1880 - val_accuracy: 0.5041\n",
            "Epoch 82/100\n",
            "750/750 [==============================] - 39s 53ms/step - loss: 1.9834 - accuracy: 0.5471 - val_loss: 2.2265 - val_accuracy: 0.4982\n",
            "Epoch 83/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9773 - accuracy: 0.5498 - val_loss: 2.1968 - val_accuracy: 0.5017\n",
            "Epoch 84/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9712 - accuracy: 0.5501 - val_loss: 2.1915 - val_accuracy: 0.5058\n",
            "Epoch 85/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9718 - accuracy: 0.5509 - val_loss: 2.2055 - val_accuracy: 0.5038\n",
            "Epoch 86/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9525 - accuracy: 0.5572 - val_loss: 2.2160 - val_accuracy: 0.4990\n",
            "Epoch 87/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9538 - accuracy: 0.5572 - val_loss: 2.1910 - val_accuracy: 0.5012\n",
            "Epoch 88/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9482 - accuracy: 0.5573 - val_loss: 2.2025 - val_accuracy: 0.4997\n",
            "Epoch 89/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9392 - accuracy: 0.5575 - val_loss: 2.1871 - val_accuracy: 0.5073\n",
            "Epoch 90/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9341 - accuracy: 0.5610 - val_loss: 2.1672 - val_accuracy: 0.5102\n",
            "Epoch 91/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 1.9324 - accuracy: 0.5599 - val_loss: 2.1859 - val_accuracy: 0.5042\n",
            "Epoch 92/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9165 - accuracy: 0.5639 - val_loss: 2.1562 - val_accuracy: 0.5124\n",
            "Epoch 93/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9109 - accuracy: 0.5667 - val_loss: 2.1752 - val_accuracy: 0.5142\n",
            "Epoch 94/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 1.9137 - accuracy: 0.5671 - val_loss: 2.1676 - val_accuracy: 0.5132\n",
            "Epoch 95/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 1.9045 - accuracy: 0.5682 - val_loss: 2.1641 - val_accuracy: 0.5136\n",
            "Epoch 96/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.8916 - accuracy: 0.5750 - val_loss: 2.1921 - val_accuracy: 0.5071\n",
            "Epoch 97/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 1.8953 - accuracy: 0.5724 - val_loss: 2.1704 - val_accuracy: 0.5090\n",
            "Epoch 98/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 1.8854 - accuracy: 0.5737 - val_loss: 2.1786 - val_accuracy: 0.5117\n",
            "Epoch 99/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.8894 - accuracy: 0.5702 - val_loss: 2.1706 - val_accuracy: 0.5090\n",
            "Epoch 100/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 1.8749 - accuracy: 0.5729 - val_loss: 2.1605 - val_accuracy: 0.5122\n",
            "cifar100_down_False_BFalse_LR0.0001.keras\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.1421 - accuracy: 0.5206\n",
            "Test Accuracy: 52.06%\n",
            "polarity: down    amsgrad: True\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization_12 (Ba  (None, 32, 32, 3)         12        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 128)       3456      \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (None, 32, 32, 128)       512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPoolin  (None, 16, 16, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 64)        73728     \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_14 (Ba  (None, 16, 16, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPooli  (None, 8, 8, 64)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 8, 8, 32)          18432     \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 8, 8, 32)          128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 4, 4, 32)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 4, 4, 32)          0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 100)               12900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 273648 (1.04 MB)\n",
            "Trainable params: 273194 (1.04 MB)\n",
            "Non-trainable params: 454 (1.77 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "750/750 [==============================] - 43s 55ms/step - loss: 8.4401 - accuracy: 0.0473 - val_loss: 7.1891 - val_accuracy: 0.0990\n",
            "Epoch 2/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 6.3084 - accuracy: 0.1204 - val_loss: 5.6601 - val_accuracy: 0.1558\n",
            "Epoch 3/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 5.1455 - accuracy: 0.1685 - val_loss: 4.7843 - val_accuracy: 0.1941\n",
            "Epoch 4/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 4.4711 - accuracy: 0.2087 - val_loss: 4.2723 - val_accuracy: 0.2247\n",
            "Epoch 5/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 4.0567 - accuracy: 0.2355 - val_loss: 3.9251 - val_accuracy: 0.2437\n",
            "Epoch 6/100\n",
            "750/750 [==============================] - 42s 56ms/step - loss: 3.7878 - accuracy: 0.2603 - val_loss: 3.7223 - val_accuracy: 0.2634\n",
            "Epoch 7/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 3.5874 - accuracy: 0.2785 - val_loss: 3.5130 - val_accuracy: 0.2903\n",
            "Epoch 8/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 3.4380 - accuracy: 0.2951 - val_loss: 3.3695 - val_accuracy: 0.2958\n",
            "Epoch 9/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 3.3157 - accuracy: 0.3121 - val_loss: 3.2457 - val_accuracy: 0.3220\n",
            "Epoch 10/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 3.2141 - accuracy: 0.3238 - val_loss: 3.1858 - val_accuracy: 0.3284\n",
            "Epoch 11/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 3.1336 - accuracy: 0.3322 - val_loss: 3.1095 - val_accuracy: 0.3392\n",
            "Epoch 12/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 3.0638 - accuracy: 0.3427 - val_loss: 3.0279 - val_accuracy: 0.3456\n",
            "Epoch 13/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 3.0068 - accuracy: 0.3516 - val_loss: 2.9690 - val_accuracy: 0.3561\n",
            "Epoch 14/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.9464 - accuracy: 0.3583 - val_loss: 2.9800 - val_accuracy: 0.3474\n",
            "Epoch 15/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.9060 - accuracy: 0.3663 - val_loss: 2.8961 - val_accuracy: 0.3613\n",
            "Epoch 16/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.8654 - accuracy: 0.3701 - val_loss: 2.9074 - val_accuracy: 0.3582\n",
            "Epoch 17/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.8227 - accuracy: 0.3773 - val_loss: 2.8409 - val_accuracy: 0.3697\n",
            "Epoch 18/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.7940 - accuracy: 0.3825 - val_loss: 2.7578 - val_accuracy: 0.3860\n",
            "Epoch 19/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.7680 - accuracy: 0.3855 - val_loss: 2.7628 - val_accuracy: 0.3814\n",
            "Epoch 20/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.7314 - accuracy: 0.3931 - val_loss: 2.7146 - val_accuracy: 0.3966\n",
            "Epoch 21/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.7057 - accuracy: 0.3974 - val_loss: 2.7263 - val_accuracy: 0.3883\n",
            "Epoch 22/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.6789 - accuracy: 0.4020 - val_loss: 2.6850 - val_accuracy: 0.3957\n",
            "Epoch 23/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.6565 - accuracy: 0.4075 - val_loss: 2.6788 - val_accuracy: 0.4030\n",
            "Epoch 24/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.6358 - accuracy: 0.4078 - val_loss: 2.6363 - val_accuracy: 0.4097\n",
            "Epoch 25/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.6156 - accuracy: 0.4126 - val_loss: 2.6187 - val_accuracy: 0.4129\n",
            "Epoch 26/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.5945 - accuracy: 0.4154 - val_loss: 2.6013 - val_accuracy: 0.4154\n",
            "Epoch 27/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.5766 - accuracy: 0.4199 - val_loss: 2.5943 - val_accuracy: 0.4138\n",
            "Epoch 28/100\n",
            "750/750 [==============================] - 42s 55ms/step - loss: 2.5581 - accuracy: 0.4251 - val_loss: 2.6040 - val_accuracy: 0.4075\n",
            "Epoch 29/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.5477 - accuracy: 0.4246 - val_loss: 2.5955 - val_accuracy: 0.4118\n",
            "Epoch 30/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.5243 - accuracy: 0.4298 - val_loss: 2.5911 - val_accuracy: 0.4149\n",
            "Epoch 31/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.5063 - accuracy: 0.4344 - val_loss: 2.5608 - val_accuracy: 0.4206\n",
            "Epoch 32/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.4922 - accuracy: 0.4354 - val_loss: 2.5398 - val_accuracy: 0.4223\n",
            "Epoch 33/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.4748 - accuracy: 0.4397 - val_loss: 2.5318 - val_accuracy: 0.4198\n",
            "Epoch 34/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.4647 - accuracy: 0.4427 - val_loss: 2.5250 - val_accuracy: 0.4288\n",
            "Epoch 35/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.4551 - accuracy: 0.4418 - val_loss: 2.4945 - val_accuracy: 0.4300\n",
            "Epoch 36/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.4382 - accuracy: 0.4449 - val_loss: 2.5246 - val_accuracy: 0.4215\n",
            "Epoch 37/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.4257 - accuracy: 0.4487 - val_loss: 2.4713 - val_accuracy: 0.4379\n",
            "Epoch 38/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.4132 - accuracy: 0.4529 - val_loss: 2.4665 - val_accuracy: 0.4337\n",
            "Epoch 39/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.3983 - accuracy: 0.4552 - val_loss: 2.4421 - val_accuracy: 0.4431\n",
            "Epoch 40/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.3868 - accuracy: 0.4577 - val_loss: 2.4272 - val_accuracy: 0.4457\n",
            "Epoch 41/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.3787 - accuracy: 0.4597 - val_loss: 2.4208 - val_accuracy: 0.4508\n",
            "Epoch 42/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.3636 - accuracy: 0.4626 - val_loss: 2.4217 - val_accuracy: 0.4497\n",
            "Epoch 43/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.3565 - accuracy: 0.4649 - val_loss: 2.4280 - val_accuracy: 0.4414\n",
            "Epoch 44/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.3490 - accuracy: 0.4655 - val_loss: 2.4144 - val_accuracy: 0.4469\n",
            "Epoch 45/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.3294 - accuracy: 0.4685 - val_loss: 2.4157 - val_accuracy: 0.4514\n",
            "Epoch 46/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.3254 - accuracy: 0.4680 - val_loss: 2.4155 - val_accuracy: 0.4479\n",
            "Epoch 47/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.3143 - accuracy: 0.4745 - val_loss: 2.4179 - val_accuracy: 0.4490\n",
            "Epoch 48/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.3022 - accuracy: 0.4752 - val_loss: 2.3806 - val_accuracy: 0.4546\n",
            "Epoch 49/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2974 - accuracy: 0.4751 - val_loss: 2.3660 - val_accuracy: 0.4638\n",
            "Epoch 50/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2843 - accuracy: 0.4780 - val_loss: 2.4031 - val_accuracy: 0.4481\n",
            "Epoch 51/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2802 - accuracy: 0.4812 - val_loss: 2.3727 - val_accuracy: 0.4570\n",
            "Epoch 52/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2742 - accuracy: 0.4799 - val_loss: 2.3784 - val_accuracy: 0.4551\n",
            "Epoch 53/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2614 - accuracy: 0.4815 - val_loss: 2.3423 - val_accuracy: 0.4643\n",
            "Epoch 54/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2444 - accuracy: 0.4878 - val_loss: 2.3599 - val_accuracy: 0.4642\n",
            "Epoch 55/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2437 - accuracy: 0.4851 - val_loss: 2.3419 - val_accuracy: 0.4666\n",
            "Epoch 56/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2365 - accuracy: 0.4885 - val_loss: 2.3494 - val_accuracy: 0.4672\n",
            "Epoch 57/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2289 - accuracy: 0.4897 - val_loss: 2.3450 - val_accuracy: 0.4651\n",
            "Epoch 58/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.2178 - accuracy: 0.4943 - val_loss: 2.3304 - val_accuracy: 0.4696\n",
            "Epoch 59/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 2.2055 - accuracy: 0.4971 - val_loss: 2.3176 - val_accuracy: 0.4736\n",
            "Epoch 60/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.2055 - accuracy: 0.4951 - val_loss: 2.3453 - val_accuracy: 0.4663\n",
            "Epoch 61/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 2.1964 - accuracy: 0.4969 - val_loss: 2.3537 - val_accuracy: 0.4634\n",
            "Epoch 62/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1890 - accuracy: 0.4978 - val_loss: 2.3030 - val_accuracy: 0.4759\n",
            "Epoch 63/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1842 - accuracy: 0.5015 - val_loss: 2.2930 - val_accuracy: 0.4758\n",
            "Epoch 64/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1711 - accuracy: 0.5034 - val_loss: 2.3428 - val_accuracy: 0.4661\n",
            "Epoch 65/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1592 - accuracy: 0.5061 - val_loss: 2.2978 - val_accuracy: 0.4783\n",
            "Epoch 66/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1642 - accuracy: 0.5046 - val_loss: 2.3061 - val_accuracy: 0.4741\n",
            "Epoch 67/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1524 - accuracy: 0.5089 - val_loss: 2.2835 - val_accuracy: 0.4840\n",
            "Epoch 68/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1492 - accuracy: 0.5091 - val_loss: 2.2940 - val_accuracy: 0.4742\n",
            "Epoch 69/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1415 - accuracy: 0.5119 - val_loss: 2.3027 - val_accuracy: 0.4754\n",
            "Epoch 70/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.1328 - accuracy: 0.5118 - val_loss: 2.2723 - val_accuracy: 0.4836\n",
            "Epoch 71/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1307 - accuracy: 0.5100 - val_loss: 2.2779 - val_accuracy: 0.4794\n",
            "Epoch 72/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.1216 - accuracy: 0.5143 - val_loss: 2.2692 - val_accuracy: 0.4820\n",
            "Epoch 73/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.1153 - accuracy: 0.5200 - val_loss: 2.2713 - val_accuracy: 0.4833\n",
            "Epoch 74/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.1153 - accuracy: 0.5157 - val_loss: 2.2748 - val_accuracy: 0.4824\n",
            "Epoch 75/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.1007 - accuracy: 0.5210 - val_loss: 2.2776 - val_accuracy: 0.4826\n",
            "Epoch 76/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.1015 - accuracy: 0.5184 - val_loss: 2.2591 - val_accuracy: 0.4862\n",
            "Epoch 77/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 2.0885 - accuracy: 0.5224 - val_loss: 2.2820 - val_accuracy: 0.4818\n",
            "Epoch 78/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0785 - accuracy: 0.5236 - val_loss: 2.2542 - val_accuracy: 0.4863\n",
            "Epoch 79/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.0762 - accuracy: 0.5253 - val_loss: 2.2664 - val_accuracy: 0.4853\n",
            "Epoch 80/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0795 - accuracy: 0.5257 - val_loss: 2.2361 - val_accuracy: 0.4930\n",
            "Epoch 81/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.0620 - accuracy: 0.5313 - val_loss: 2.2590 - val_accuracy: 0.4863\n",
            "Epoch 82/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0638 - accuracy: 0.5257 - val_loss: 2.2545 - val_accuracy: 0.4837\n",
            "Epoch 83/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.0476 - accuracy: 0.5355 - val_loss: 2.2499 - val_accuracy: 0.4886\n",
            "Epoch 84/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0436 - accuracy: 0.5341 - val_loss: 2.2307 - val_accuracy: 0.4962\n",
            "Epoch 85/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.0499 - accuracy: 0.5318 - val_loss: 2.2375 - val_accuracy: 0.4938\n",
            "Epoch 86/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.0426 - accuracy: 0.5350 - val_loss: 2.2693 - val_accuracy: 0.4812\n",
            "Epoch 87/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.0331 - accuracy: 0.5372 - val_loss: 2.2478 - val_accuracy: 0.4926\n",
            "Epoch 88/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 2.0286 - accuracy: 0.5384 - val_loss: 2.2451 - val_accuracy: 0.4895\n",
            "Epoch 89/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0240 - accuracy: 0.5393 - val_loss: 2.2346 - val_accuracy: 0.4928\n",
            "Epoch 90/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0145 - accuracy: 0.5427 - val_loss: 2.2278 - val_accuracy: 0.4900\n",
            "Epoch 91/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 2.0154 - accuracy: 0.5410 - val_loss: 2.2214 - val_accuracy: 0.4968\n",
            "Epoch 92/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 2.0061 - accuracy: 0.5414 - val_loss: 2.2279 - val_accuracy: 0.4950\n",
            "Epoch 93/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 1.9983 - accuracy: 0.5470 - val_loss: 2.2389 - val_accuracy: 0.4928\n",
            "Epoch 94/100\n",
            "750/750 [==============================] - 41s 55ms/step - loss: 1.9976 - accuracy: 0.5454 - val_loss: 2.2316 - val_accuracy: 0.4962\n",
            "Epoch 95/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 1.9957 - accuracy: 0.5446 - val_loss: 2.2234 - val_accuracy: 0.4966\n",
            "Epoch 96/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 1.9921 - accuracy: 0.5474 - val_loss: 2.2176 - val_accuracy: 0.4993\n",
            "Epoch 97/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 1.9813 - accuracy: 0.5497 - val_loss: 2.2341 - val_accuracy: 0.4925\n",
            "Epoch 98/100\n",
            "750/750 [==============================] - 40s 54ms/step - loss: 1.9820 - accuracy: 0.5506 - val_loss: 2.2301 - val_accuracy: 0.4925\n",
            "Epoch 99/100\n",
            "750/750 [==============================] - 40s 53ms/step - loss: 1.9723 - accuracy: 0.5518 - val_loss: 2.2430 - val_accuracy: 0.4910\n",
            "Epoch 100/100\n",
            "750/750 [==============================] - 41s 54ms/step - loss: 1.9643 - accuracy: 0.5543 - val_loss: 2.2075 - val_accuracy: 0.4980\n",
            "cifar100_down_True_BFalse_LR0.0001.keras\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.1866 - accuracy: 0.4984\n",
            "Test Accuracy: 49.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show files\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-S648Mc_KBx",
        "outputId": "4ae19f99-754e-49bc-bfa1-323510af3254"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cifar100_down_False_BFalse_LR0.0001.keras  cifar100_up_True_BFalse_LR0.0001.keras  __pycache__\n",
            "cifar100_down_True_BFalse_LR0.0001.keras   main.py\t\t\t\t   sample_data\n",
            "cifar100_up_False_BFalse_LR0.0001.keras    model.py\t\t\t\t   util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the downloading in files is bugged for colab\n",
        "# so this command is separate from saving\n",
        "\n",
        "#files.download(\"cifar100_down_False_BFalse_LR0.001.keras\")\n",
        "#files.download(\"cifar100_down_True_BFalse_LR0.001.keras\")\n",
        "#files.download(\"cifar100_up_False_BFalse_LR0.001.keras\")\n",
        "#files.download(\"cifar100_up_True_BFalse_LR0.001.keras\")\n",
        "\n",
        "#files.download(\"cifar100_down_False_BTrue_LR0.001.keras\")\n",
        "#files.download(\"cifar100_down_True_BTrue_LR0.001.keras\")\n",
        "#files.download(\"cifar100_up_False_BTrue_LR0.001.keras\")\n",
        "#files.download(\"cifar100_up_True_BTrue_LR0.001.keras\")\n",
        "\n",
        "files.download(\"cifar100_down_False_BFalse_LR0.0001.keras\")\n",
        "files.download(\"cifar100_down_True_BFalse_LR0.0001.keras\")\n",
        "files.download(\"cifar100_up_False_BFalse_LR0.0001.keras\")\n",
        "files.download(\"cifar100_up_True_BFalse_LR0.0001.keras\")\n",
        "\n",
        "#files.download(\"cifar100_down_False_BTrue_LR0.0001.keras\")\n",
        "#files.download(\"cifar100_down_True_BTrue_LR0.0001.keras\")\n",
        "#files.download(\"cifar100_up_False_BTrue_LR0.0001.keras\")\n",
        "#files.download(\"cifar100_up_True_BTrue_LR0.0001.keras\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YGJmaujFALW9",
        "outputId": "15759e23-f8f9-431f-b32f-9481460466c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a000dc19-f4a1-4778-a09a-9615932a5cc4\", \"cifar100_down_False_BFalse_LR0.0001.keras\", 3361248)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_64e8e9fa-d898-494c-b419-5206b19de133\", \"cifar100_down_True_BFalse_LR0.0001.keras\", 4460323)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e5b308a3-3650-4b71-a480-0b0894b5e552\", \"cifar100_up_False_BFalse_LR0.0001.keras\", 8048719)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8da761c5-8c8d-4822-8fd9-8e0ba92e14e3\", \"cifar100_up_True_BFalse_LR0.0001.keras\", 10710294)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}